import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
import joblib
import matplotlib.pyplot as plt

def tune_knn(X_train, y_train, k_values=range(1, 31), cv=5):
    print("üîç Tuning K cho KNN...")
    cv_scores = []

    for k in k_values:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X_train, y_train, cv=cv, scoring='accuracy')
        mean_score = scores.mean()
        cv_scores.append(mean_score)
        print(f"k={k} => CV Accuracy: {mean_score:.4f}")

    best_k = k_values[np.argmax(cv_scores)]
    print(f"‚úÖ K t·ªët nh·∫•t: {best_k} v·ªõi accuracy CV = {max(cv_scores):.4f}")

    # Plot k·∫øt qu·∫£
    plt.figure(figsize=(10,6))
    plt.plot(k_values, cv_scores, marker='o')
    plt.xlabel('S·ªë l∆∞·ª£ng l√°ng gi·ªÅng k')
    plt.ylabel('Accuracy (CV)')
    plt.title('Tuning s·ªë l∆∞·ª£ng l√°ng gi·ªÅng K trong KNN')
    plt.grid(True)
    plt.show()

    return best_k

def evaluate_model(model, X_test, y_test, model_name="Model"):
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nüìä B√°o c√°o ƒë√°nh gi√° cho {model_name}:")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

def check_rf_overfitting(rf_model, X_train, y_train, X_test, y_test):
    train_acc = rf_model.score(X_train, y_train)
    test_acc = rf_model.score(X_test, y_test)
    print("\nüå≤ Ki·ªÉm tra Overfitting c·ªßa Random Forest:")
    print(f"Accuracy tr√™n t·∫≠p hu·∫•n luy·ªán: {train_acc:.4f}")
    print(f"Accuracy tr√™n t·∫≠p ki·ªÉm tra: {test_acc:.4f}")

    if train_acc - test_acc > 0.05:
        print("‚ö†Ô∏è C√≥ d·∫•u hi·ªáu Overfitting (ch√™nh l·ªách > 5%)")
    else:
        print("‚úÖ Kh√¥ng c√≥ d·∫•u hi·ªáu overfitting r√µ r√†ng")


def plot_learning_curve(estimator, X, y, title="Learning Curve", cv=5, scoring='accuracy'):

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, scoring=scoring,
        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1, random_state=42
    )

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.figure(figsize=(10, 6))
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel(scoring)
    plt.grid(True)

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")

    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    plt.legend(loc="best")
    plt.show()

if __name__ == "__main__":
    # Load data
    df = pd.read_csv("../data/weather_data_processed.csv")

    # Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ c√°c c·ªôt t√°ch Day, Month, Year, Hour nh∆∞ ƒë√£ n√≥i
    # T·∫°o feature v√† label
    feature_cols = ['Day', 'Month', 'Year', 'Hour', 'Temp_C', 'Humidity_pct', 'Precipitation_mm', 'WindSpeed_kmh', 'CloudCover_pct', 'AtmosPressure_hPa']
    target_col = 'WeatherCondition'

    X = df[feature_cols]
    y = df[target_col]

    # Chia train-test (vd nƒÉm < 2024 l√† train, nƒÉm = 2024 l√† test)
    train_df = df[df['Year'] < 2024]
    test_df = df[df['Year'] == 2024]

    X_train = train_df[feature_cols]
    y_train = train_df[target_col]
    X_test = test_df[feature_cols]
    y_test = test_df[target_col]

    # --- Tune KNN ---
    best_k = tune_knn(X_train, y_train)

    # Train KNN v·ªõi k t·ªët nh·∫•t
    knn = KNeighborsClassifier(n_neighbors=best_k)
    knn.fit(X_train, y_train)
    evaluate_model(knn, X_test, y_test, "KNN")

    # Train Random Forest v·ªõi default (b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh params)
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_train, y_train)
    evaluate_model(rf, X_test, y_test, "Random Forest")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    plot_learning_curve(rf, X_train, y_train, title="Learning Curve - Random Forest")

    # Ki·ªÉm tra overfitting c·ªßa RF
    check_rf_overfitting(rf, X_train, y_train, X_test, y_test)

    # L∆∞u model n·∫øu mu·ªën
    joblib.dump(knn, "../saved_models/knn_weather_model.pkl")
    joblib.dump(rf, "../saved_models/rf_weather_model.pkl")

    print("\nüéâ Ho√†n th√†nh ƒë√°nh gi√° v√† l∆∞u model!")
